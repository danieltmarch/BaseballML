{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "injured-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-maldives",
   "metadata": {},
   "source": [
    "### Split Data into train, val, test\n",
    "Val and Test make up about 1/6 of the data each\n",
    "This portion should only be ran once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparative-failing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48555\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(\"F:\\\\Users\\\\Daniel\\\\Machine Learning Work\\\\Baseball work\\\\Game Log Data\\\\BaseballData.txt\", \"r\") as dataFile:\n",
    "    for line in dataFile:\n",
    "        data.append(line)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fewer-contest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32555\n",
      "8000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "testSize = 8000\n",
    "valSize = 8000\n",
    "trainSize = len(data) - testSize - valSize\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "trainData = data[0:trainSize]\n",
    "valData = data[trainSize: trainSize + valSize]\n",
    "testData = data[trainSize + valSize :]\n",
    "print(len(trainData))\n",
    "print(len(valData))\n",
    "print(len(testData))\n",
    "\n",
    "with open(\"F:\\\\Users\\\\Daniel\\\\Machine Learning Work\\\\Baseball work\\\\ML Data\\\\train.txt\", \"w\") as trainFile:\n",
    "    for line in trainData:\n",
    "        trainFile.write(line)\n",
    "        \n",
    "with open(\"F:\\\\Users\\\\Daniel\\\\Machine Learning Work\\\\Baseball work\\\\ML Data\\\\val.txt\", \"w\") as valFile:\n",
    "    for line in valData:\n",
    "        valFile.write(line)\n",
    "        \n",
    "with open(\"F:\\\\Users\\\\Daniel\\\\Machine Learning Work\\\\Baseball work\\\\ML Data\\\\test.txt\", \"w\") as testFile:\n",
    "    for line in testData:\n",
    "        testFile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-biotechnology",
   "metadata": {},
   "source": [
    "### Read/Load the data and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "relative-omega",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32555, 474)\n"
     ]
    }
   ],
   "source": [
    "def normalizePitcherData(data):\n",
    "    for i in range(len(data)):\n",
    "        #note normalizing here puts most values around the -1 to 1 range but many fields don't strictly follow this\n",
    "        if(data[i][0] != 0):\n",
    "            data[i][1:26] = data[i][1:26]/data[i][0] #what percentage of pitched games this happened / how many times per pitched game\n",
    "        data[i][0] = data[i][0]/162 #what percentage of games the player pitches in\n",
    "        #26,27,28,29 don't need to be changed\n",
    "    return data\n",
    "    \n",
    "def normalizeBatterData(data):\n",
    "    for i in range(len(data)):\n",
    "        #note normalizing here puts most values around the -1 to 1 range but many fields don't strictly follow this\n",
    "        if(data[i][1] != 0):\n",
    "            data[i][2:19] = data[i][2:19]/data[i][1] #get average per at bat\n",
    "        if(data[i][0] != 0):\n",
    "            data[i][1] = data[i][1]/data[i][0] #avr. bats per batted game\n",
    "        data[i][0] = data[i][0]/162 #shows what percent of games the player batts in\n",
    "        #19,20,21,22 don't need to be changed\n",
    "    return data\n",
    "\n",
    "def getDataFromFile(fileName):\n",
    "    fileData=np.loadtxt(fileName, delimiter=',') \n",
    "    [N,dim]=np.shape(fileData) #set up matrix from file\n",
    "    \n",
    "    dataX = np.zeros((N, dim))\n",
    "    dataY = np.zeros((N))\n",
    "    \n",
    "    dataX = fileData[:, 0:dim - 1]\n",
    "    dataY[:] = fileData[:, dim - 1]\n",
    "    \n",
    "    startIndex = 0\n",
    "    #format of dataX:\n",
    "    #home pitcher data (30 fields)\n",
    "    dataX[:, startIndex:startIndex + 30] = normalizePitcherData(dataX[:, startIndex:startIndex + 30])\n",
    "    startIndex = startIndex + 30\n",
    "    #home batter 1 (23 fields)\n",
    "    #home batter 2 (23 fields)\n",
    "    #...\n",
    "    #home batter 9 (23 fields)\n",
    "    for i in range(9):\n",
    "        dataX[:, startIndex:startIndex + 23] = normalizeBatterData(dataX[:, startIndex:startIndex + 23])\n",
    "        startIndex = startIndex + 23\n",
    "    \n",
    "    #away pitcher data (30 fields)\n",
    "    dataX[:, startIndex:startIndex + 30] = normalizePitcherData(dataX[:, startIndex:startIndex + 30])\n",
    "    startIndex = startIndex + 30\n",
    "    #away batter 1 (23 fields)\n",
    "    #away batter 2 (23 fields)\n",
    "    #...\n",
    "    #away batter 9 (23 fields)\n",
    "    for i in range(9):\n",
    "        dataX[:, startIndex:startIndex + 23] = normalizeBatterData(dataX[:, startIndex:startIndex + 23])\n",
    "        startIndex = startIndex + 23\n",
    "    \n",
    "    #for pitcher data: divide all fields by 162 to normalize\n",
    "    return dataX, dataY\n",
    "    \n",
    "trainFileName = \"F:\\\\Users\\\\Daniel\\\\Machine Learning Work\\\\Baseball work\\\\ML Data\\\\train.txt\"\n",
    "valFileName = \"F:\\\\Users\\\\Daniel\\\\Machine Learning Work\\\\Baseball work\\\\ML Data\\\\val.txt\"\n",
    "#testFileName = \"F:\\\\Users\\\\Daniel\\\\Machine Learning Work\\\\Baseball work\\\\ML Data\\\\test.txt\"\n",
    "\n",
    "trainX, trainY = getDataFromFile(trainFileName)\n",
    "valX, valY = getDataFromFile(valFileName)\n",
    "#testX, testY = getDataFromFile(testFileName)#\n",
    "\n",
    "print(np.shape(trainX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-essex",
   "metadata": {},
   "source": [
    "### Setup NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prospective-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lamda=0.001: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2fa3cd2c20a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mepochCount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"For lamda=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlamda\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0meIn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meOut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelInter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0meInList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meIn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#training error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0meOutList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meOut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#validation error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-96b90bcb85cf>\u001b[0m in \u001b[0;36mrunModel\u001b[1;34m(trainX, trainY, testX, testY, model, epochCount)\u001b[0m\n\u001b[0;32m      5\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mhistoryData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochCount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0meIn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistoryData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0meOut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistoryData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lamdaList = [.001] #[10, 1, .1, .05, .01, .005, .001, .0005, .00001, 0]\n",
    "eInList = []\n",
    "eOutList = []\n",
    "for lamda in lamdaList:\n",
    "    regParam = keras.regularizers.l2(lamda) #create regularization parameter\n",
    "    \n",
    "    modelInter = tf.keras.Sequential([\n",
    "            keras.layers.Dense(units=np.shape(trainX)[1], activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=600, activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=800, activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=1000, activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=800, activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=400, activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=200, activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=100, activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=25, activation='linear', kernel_regularizer=regParam),\n",
    "            keras.layers.Dense(units=1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    epochCount = 50\n",
    "    print(\"For lamda=\" + str(lamda) + \": \")\n",
    "    eIn, eOut = runModel(trainX, trainY, valX, valY, modelInter, epochCount)\n",
    "    eInList.append(eIn[-1]) #training error\n",
    "    eOutList.append(eOut[-1]) #validation error\n",
    "    #print(\"Ein:\", eIn)\n",
    "    #print(\"Eout:\", eOut)\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "ax.plot(lamdaList, eInList,'-b')\n",
    "ax.plot(lamdaList, eOutList,'-r')\n",
    "ax.set_xlabel(\"Lamda\")\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_title(\"Lamda and Error\")\n",
    "ax.grid()\n",
    "ax.legend(['eIn', 'eVal'])\n",
    "ax.set_xscale(\"log\")\n",
    "ax.axes.set_ylim(bottom = .4, top = .5) #all errors should be within this range\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aging-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(trainX, trainY, testX, testY, model, epochCount, verbose=0): #run and graph model results\n",
    "    #compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    historyData = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=epochCount, verbose=verbose)\n",
    "    eIn = historyData.history[\"accuracy\"]\n",
    "    eOut = historyData.history[\"val_accuracy\"]\n",
    "    \n",
    "    for i in range(len(eIn)):\n",
    "        eIn[i] = 1-eIn[i]\n",
    "    for i in range(len(eOut)):\n",
    "        eOut[i] = 1-eOut[i]\n",
    "\n",
    "    print('Train error:', eIn[-1])\n",
    "    print('Test error: ', eOut[-1])\n",
    "    \n",
    "    #return error data to be plot later\n",
    "    return eIn, eOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "rotary-belle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.41737496852874756\n",
      "Test erorr:  0.4082949161529541\n",
      "Train error: 0.41010409593582153\n",
      "Test erorr:  0.40522271394729614\n",
      "Train error: 0.4088069796562195\n",
      "Test erorr:  0.4165898561477661\n",
      "Train error: 0.4079194664955139\n",
      "Test erorr:  0.41843318939208984\n",
      "Train error: 0.40754395723342896\n",
      "Test erorr:  0.41198158264160156\n",
      "Train error: 0.4103430509567261\n",
      "Test erorr:  0.406144380569458\n",
      "Train error: 0.4073391556739807\n",
      "Test erorr:  0.4153609871864319\n",
      "Train error: 0.4082608222961426\n",
      "Test erorr:  0.4291858673095703\n",
      "Train error: 0.4082266688346863\n",
      "Test erorr:  0.41628265380859375\n",
      "Train error: 0.4100358486175537\n",
      "Test erorr:  0.41904759407043457\n",
      "Avr eIn: 0.4095955014228821\n",
      "Avr eVal: 0.41465437412261963\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "segSize = int(len(trainX)/k)\n",
    "kTrainX = np.zeros(( (k-1) * segSize, np.shape(trainX)[1]))\n",
    "kTrainY = np.zeros(( (k-1) * segSize))\n",
    "\n",
    "\n",
    "regParam = keras.regularizers.l2(.0001) #create regularization parameter\n",
    "#create the same type of model for kfold testing\n",
    "modelFold = tf.keras.Sequential([\n",
    "        keras.layers.Dense(units=np.shape(trainX)[1], activation='linear', kernel_regularizer=regParam),\n",
    "        keras.layers.Dense(units=200, activation='linear', kernel_regularizer=regParam),\n",
    "        keras.layers.Dense(units=100, activation='linear', kernel_regularizer=regParam),\n",
    "        keras.layers.Dense(units=25, activation='linear', kernel_regularizer=regParam),\n",
    "        keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "eInSum = 0\n",
    "eOutSum = 0\n",
    "for i in range(k): #denotes which segment is the validation segment\n",
    "    for j in range(k): #setup the data sets\n",
    "        if(j < i): #copy data over to k data\n",
    "            kTrainX[segSize*(j) : segSize*(j+1)] = trainX[segSize*j : segSize*(j+1)]\n",
    "            kTrainY[segSize*(j) : segSize*(j+1)] = trainY[segSize*j : segSize*(j+1)]\n",
    "        elif(j == i): #this is the val segment\n",
    "            kValX = trainX[segSize*j : segSize*(j+1)]\n",
    "            kValY = trainY[segSize*j : segSize*(j+1)]\n",
    "        else: #j > i\n",
    "            kTrainX[segSize*(j-1) : segSize*(j)] = trainX[segSize*j : segSize*(j+1)]\n",
    "            kTrainY[segSize*(j-1) : segSize*(j)] = trainY[segSize*j : segSize*(j+1)]\n",
    "    \n",
    "    epochCount = 75 #75 iter\n",
    "    eIn, eOut = runModel(kTrainX, kTrainY, kValX, kValY, modelFold, epochCount)\n",
    "    eInSum = eInSum + eIn[-1]\n",
    "    eOutSum = eOutSum + eOut[-1]\n",
    "    \n",
    "print(\"Avr eIn:\", eInSum / k)\n",
    "print(\"Avr eVal:\", eOutSum / k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "committed-iraqi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.4093380570411682\n",
      "Test erorr:  0.41212499141693115\n"
     ]
    }
   ],
   "source": [
    "epochCount = 200 #200 iter\n",
    "eIn, eOut = runModel(trainX, trainY, valX, valY, modelFold, epochCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "heavy-observer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_modelCheckpoint2\\assets\n"
     ]
    }
   ],
   "source": [
    "#save model, from tensorflow documentation, https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\n",
    "!mkdir -p saved_model\n",
    "modelFold.save('saved_model/my_modelCheckpoint2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "balanced-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model, also from tensorflow documentation\n",
    "new_model = tf.keras.models.load_model('saved_model/my_modelCheckpoint2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
